{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halite challenge - advanced topics\n",
    "\n",
    "We have seen how to solve the reinforcement learning problem of a single ship that has to optimize the collection of halite. We now want to extend the learning process to multiple ships. To do that we face two new kind of problems/tasks in addition to the one already solved for the single ship case. The first one is how to teach to the shipyard to spawn efficiently the ships. The second one is how to coordinate all the ships at each turn and also in the learning process.\n",
    "\n",
    "#  Multi-agent framework\n",
    "\n",
    "Two classes of agents: ship and shipyard. Only one instance of shipyard agent, many of the ship agent.\n",
    "\n",
    "### Challenges:\n",
    "\n",
    "1. Interface all the ship agents in order to make them learn withouth crashing into each other;\n",
    "2. Define for each class of agents what is the (partial) state that they observe at each turn;\n",
    "3. Shape the reward for multiple ships (one for all or one for each?) and for the shipyard (just a single reward for all the episode?);\n",
    "4. Choose how to iteratively train the two classes whithout making confiusion that could ruin the training procedure, e.g. assigning a penalty to the shipyard for a fault of a ship agent.\n",
    "\n",
    "\n",
    "# Ship class with tabular method\n",
    "\n",
    "The simplest and maybe more straightforward way to generalize from one ship to many ships is to consider them as separate and independent entities, that share as little as possible between them so that their learning is very similar to the single ship case and that it is required less experience to train them. In fact the number of possible states of the system would grow exponentially if we were to consider as a state the union of the states of all ships and that would be infeasible even for two ships. For example for a single ship in a $7 \\times 7$ map we have 142.884 possible states; consequently we would have $(142.844)^2 = 20.4\\cdot10^9$ states. Considering that each of these states has to be multiplied for all the possible combined actions of the two ships (25) and requires 64 bits, i.e. 8 bytes, to be stored, the memory required to store the Q-value table is $4.08\\cdot 10^{12}$ bytes = 4.08 Tb, that definetively doesn't fit any existing RAM.\n",
    "\n",
    "## State representation\n",
    "\n",
    "Assessed the fact that a shared state is infeasible, what we can do is to encode in a minimal way some the information that is necessary to a ship to avoid collisions. Since the maximum range of the composed motion of two ships is 2 cells and near a ship there can be multiple ships, we can choose to encode the positions of the other ships inside a rectangle of $5\\times5$ relative to the position of the considered ship. But in this way we also include cells that are more than two moves away from the center (because diagonal moves aren't allowed).\n",
    "In the grid below the `___` are the cells in the range of the two moves, instead the `***` are the ones out of range.\n",
    "\n",
    "| / | 0| 1 | 2 | 3 | 4 |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "|0    | *** | *** | ___ | *** | *** |\n",
    "|1    | *** | ___ | ___ | ___ | *** |\n",
    "|2    | ___ | ___ | ship | ___ | ___ |\n",
    "|3    | *** | ___ | ___ | ___ | *** |\n",
    "|4    | *** | *** | ___ | *** | *** |\n",
    "\n",
    "All we need is the information about 13 out of the 25 cells. Each of these cells can be occupied (1) or empty (0), thus all the possible combinations are $2^{13} = 8192$. This results in a total number of state-actions of $5.850\\cdot 10^9$ and 47 Gb of space. Again this is too much.\n",
    "\n",
    "Thinking about the scope of this information, we can easily realize that is all about what are the dangerous directions and what are the safe ones. Since we are forced to compress this information, the minimal but still relevant information would be a single direction, the safest one. We can label dangerous a direction if taking an action in that direction has non-zero probability of colliding with another ship. If we have just one dangerous direction, the safest one is the opposite direction; if we have two, we can choose randomly one of the two safe directions; if we have three, only one direction remains and finally if all four direction are dangerous, the safest option is to remain still (this doesn't ensure the safety of the ship, but if all the ships follow the same optimal policy should work). So we can use a variable `safe_dir` $\\in [0,4]$ to encode this information. This results in in a total number of state-actions of $3.57\\cdot 10^6$ and 28.57 Mb of space. \n",
    "\n",
    "## Reward\n",
    "\n",
    "Since we choose to use independent agents for all the ships, it makes sense to decompose the reward into the contributions of all the ships. The contribution of a ship is 0 if it doesn't deposit halite (and in that case it receives the baseline reward, e.g. -0.01) and is the halite stored divided by 1000 minus the baseline reward for the ship that actually has deposited the halite.\n",
    "\n",
    "# Shipyard class\n",
    "\n",
    "Dealing with the shipyard is a completely different problem, for different reasons:\n",
    "\n",
    "1. It has at disposal a binary choice (spawn ship or stay still);\n",
    "2. It's not responsible for the absolute result of the episode, but its choices can influence it greatly;\n",
    "3. Its choices are more affected by the current time of the episode than the ship ones (spawning a ship in the last few turns it's definitively a bad idea);\n",
    "4. It receives feedback about its action with a much lower frequence than the ship agents, because the only metric to evaluate its policy is the final amount of halite collected and how it has changed through the episodes while keeping fixed the policy of the ships.\n",
    "\n",
    "If we try to understand what are the informations that the shipyard needs to have in order to decide whether to spawn a ship or not, we find:\n",
    "\n",
    "1. Current number of ships `N`;\n",
    "2. Number of turns to the end `t_left` (in this way its flexible about the total number of turns);\n",
    "3. Halite available `h_tot`;\n",
    "4. Position of the ships in the map or some variable connected to it (for example we don't want to spawn a ship while another one is dropping the halite in the shipyard, because it would cause a shipwreck).\n",
    "\n",
    "### More on the position of the ships variable\n",
    "\n",
    "We now analize more in detail how to build a variable that can contain the information about how busy is the area around the shipyard, so that the shipyard can decide not to spawn a ship when it is too busy and thus risky to do it.\n",
    "First of all, given the symmetry of the map and the fact that the action of the shipyard preserves all those symmetries (in fact the new ship is born in the shipyard cell, that is at the center of the map), this variable should assume the same value under equivalent configurations of the system.\n",
    "Second, it should be local, since the shipyard it is not much concerned about all what is happening in the map (that is the ship agent job).\n",
    "A minimal approach could be to define an area within a certain number of moves (e.g. 1 or 2) and simply count the number of ships in that area. This is a measure of the concentration of ships and consequently of the risk in spawning a new ship and seeing it crashing with another one. For the first implementation we consider an area whithin a single move from the shipyard for the counting and call the variable so obtained `near_ships`.\n",
    "\n",
    "#\n",
    "\n",
    "Since the number of variables is low and the choice it has to do is binary, it seems natural to use a function instead that a table to approximate the Q value of the choices. In other words, instead of keeping record in a table for each state s and action a of the Q value Q(s,a), we can use a function $f$ with the interpretation $f(s) = P[Q(s,1) > Q(s,0)]$, where for convention we use $a=1$ for spawning a ship and $a=0$ for staying still.\n",
    "Then we have to reformulate the task of the shipyard in order to understand how to determine the function $f$.\n",
    "Notice that this passage from a tabular record of all the state and actions to a more functional approach to the problem assumes that the dependence on each of the variables is continuous (if the function considered is continuous itself). \n",
    "\n",
    "## Critical aspects\n",
    "\n",
    "1. The Q-learning is based on a sort of chain rule to estimate the Q-values, starting from some known value (for example that of the terminal state) and propagating backwards that knowledge through the individual rewards of each state-action that forms the chain of an episode. In this particular situation it's not clear how to assign a step by step reward and would be better to find another approach to disentangle the dependence of the total reward on each single action (taken in a given state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
