{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Visualizing Learning\n",
    "\n",
    "In the previous tutorial we have seen how to train an agent, but I bet that everyone of us has thought at least once \"Is it really learning anything?\". In this tutorial we will try to answer to that question.\n",
    "\n",
    "We gathered again all the functions written in the previous tutorial in a script ship.py, that you can find in the Modules folder. \n",
    "\n",
    "## Outline\n",
    "1. Online visualization: how to see how the training is doing while the agent is still training.\n",
    "2. Rendering: how to visualize the agent moving during one episode.\n",
    "2. Post-training visualization: how to look at the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules and libraries\n",
    "import sys\n",
    "sys.path.insert(0, \"../Environment/\")\n",
    "sys.path.insert(0, \"../Modules/\")\n",
    "import halite_env as Env\n",
    "import encode as cod\n",
    "import ship "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time #used for time.sleep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redifine all variables and constants as in the previous tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@\n",
    "# Environment variables\n",
    "#@@@@@@@@@@@@@@@@@@@@@@\n",
    "NUM_PLAYERS = 1\n",
    "MAP_SIZE = 7 # 7 x 7 map\n",
    "TOT_TURNS = 400 # number of turns for each episode\n",
    "\n",
    "#@@@@@@@@@@@@@@@@\n",
    "# State variables\n",
    "#@@@@@@@@@@@@@@@@\n",
    "H_LEV = 3 # halite levels\n",
    "N_CELLS = MAP_SIZE**2 # number of cells in a square map\n",
    "N_STATES = N_CELLS*H_LEV**6*4\n",
    "N_ACTIONS = 5 # no dropoffs, 1 action for staying still, 4 for moving in the cardinal directions\n",
    "print(\"Total number of states to be experienced: \", N_STATES)\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@\n",
    "# Learning parameters\n",
    "#@@@@@@@@@@@@@@@@@@@@\n",
    "N_BATCH = 30 #100 # number of episodes in an epoch\n",
    "MAX_EPOCHS = 200 # max number of epochs played before stopping (500 ~ 7.3 hours of training)\n",
    "DISCOUNT_FACTOR = 1 - 1/TOT_TURNS #train ships as if each turn has a probability of 1/tot_turns of ending the game \n",
    "STD_REWARD = -0.01\n",
    "LEARNING_RATE = 0.1\n",
    "EPS_START = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all the parameters to dictionaries\n",
    "env_dict = dict(NUM_PLAYERS = NUM_PLAYERS , \n",
    "                TOT_TURNS = TOT_TURNS)\n",
    "\n",
    "state_dict = dict(MAP_SIZE = MAP_SIZE,  \n",
    "                  H_LEV = H_LEV, \n",
    "                  N_ACTIONS = N_ACTIONS)\n",
    "\n",
    "learning_dict = dict(LEARNING_RATE = LEARNING_RATE, \n",
    "                     DISCOUNT_FACTOR = DISCOUNT_FACTOR , \n",
    "                     eps = EPS_START, \n",
    "                     STD_REWARD = STD_REWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@\n",
    "# Learning variables\n",
    "#@@@@@@@@@@@@@@@@@@@\n",
    "q_values = np.zeros((N_STATES,N_ACTIONS)) #initialize to zero\n",
    "#q_values = np.load(\"Q_values.npy\") # or re-use the one already trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Online visualization\n",
    "\n",
    "The online visualization is based on two ideas: to find some metrics to monitor and to monitor them almost in real time.\n",
    "\n",
    "One of the obvious metrics that we measured in the previous tutorial was the return at the end of the episode.\n",
    "Another one directly connected to that, but with a more practical touch, is the halite collected from the agent during the episode.\n",
    "\n",
    "And finally we would like to know how many time the ship passes through the shipyard, since it is a good indicator of what is going on.\n",
    "\n",
    "Thus we now modify the play_episode function written in the previous tutorial in order to make it compute those quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(q_values, eps, NUM_PLAYERS, MAP_SIZE, TOT_TURNS, N_ACTIONS, H_LEV,\n",
    "                 STD_REWARD,LEARNING_RATE, DISCOUNT_FACTOR, verbose = False):\n",
    "    \"\"\"\n",
    "    Trains the agent by playing one episode of halite.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    q_values         : numpy array \n",
    "        Contains the Q-values\n",
    "    eps              : float \n",
    "        Represents a probability, must be in [0,1], controls the probability of exploring instead of exploting\n",
    "    NUM_PLAYERS      : int\n",
    "    MAP_SIZE         : int\n",
    "    TOT_TURNS        : int\n",
    "    N_ACTIONS        : int\n",
    "    H_LEV            : int\n",
    "    STD_REWARD       : float\n",
    "        Baseline reward given to the agent when does not deposit halite to the shipyard\n",
    "    LEARNING_RATE    : float\n",
    "    DISCOUNT_FACTOR  : float\n",
    "        Must be greater than 0 but smaller than 1. Suggested 1-1/TOT_TURNS or 1\n",
    "    verbose          : bool\n",
    "        Prints halite of the player at each turn of the game\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    q_values         : numpy array \n",
    "        Updated Q-values\n",
    "    reward           : float\n",
    "        Reward obtained in this episode. \n",
    "    collected_halite : float\n",
    "        Halite collected by the agent.\n",
    "    passages         : int\n",
    "        Number of passages of the ship through the shipyard.\n",
    "    \"\"\"\n",
    "    env = Env.HaliteEnv(NUM_PLAYERS, MAP_SIZE, episode_lenght = TOT_TURNS) # init environment\n",
    "    steps = 0\n",
    "    reward = 0 # cumulative reward of the episode\n",
    "    passages = 0 # number of times the ship passes through the shipyard\n",
    "    \n",
    "    # first mandatory step\n",
    "    steps = steps + 1\n",
    "    if verbose:\n",
    "        print(\"\\nStep number %d:\"%steps)\n",
    "    action_matrix = np.full((MAP_SIZE,MAP_SIZE), -1) # no ship, no action\n",
    "    shipyard_action = True # initially always choose to create a ship\n",
    "    # returns the matricial state, the array of players halite and a flag that is true if it's the final turn\n",
    "    state, players_halite, finish, _ = env.step(action_matrix, makeship = shipyard_action) \n",
    "    #print(\"Cargo layer: \\n\", state[:,:,2])\n",
    "    current_halite = players_halite[0][0]\n",
    "    s_enc = cod.encode_state(state, map_size = MAP_SIZE, h_lev = H_LEV, n_actions = N_ACTIONS, debug=False)\n",
    "\n",
    "    while True:\n",
    "        steps = steps + 1\n",
    "        if verbose:\n",
    "            print(\"\\nStep number %d:\"%steps)\n",
    "            print(\"Current halite: \", current_halite)\n",
    "        a_enc = ship.e_greedy_policy(s_enc, q_values, eps = eps)\n",
    "        a_mat = cod.scalar_to_matrix_action(a_enc, state, map_size = MAP_SIZE) #convert the action in matricial form\n",
    "\n",
    "        # submit the action and get the new state\n",
    "        state, players_halite, finish, _ = env.step(a_mat, makeship = False) \n",
    "\n",
    "        new_halite = players_halite[0][0]\n",
    "\n",
    "        # compute the 1-ship reward as the halite increment of the player divided by the max halite \n",
    "        # plus a standard negative reward \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        if new_halite == current_halite:\n",
    "            r =  STD_REWARD\n",
    "        else:\n",
    "            r = (new_halite - current_halite)/1000 + STD_REWARD*50 # this is the change in the code\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        sp_enc = cod.encode_state(state, map_size = MAP_SIZE, h_lev = H_LEV, n_actions = N_ACTIONS, debug=False)\n",
    "        reward += r # cumulative reward of the episode\n",
    "\n",
    "        # adds 1 to passages if the current position of the ship coincides with that of the shipyard\n",
    "        # whereas the previous position didn't\n",
    "        s_dec = cod.decode3D(s_enc, L1 = MAP_SIZE**2, L2 = H_LEV**6, L3 = N_ACTIONS-1)\n",
    "        sp_dec = cod.decode3D(sp_enc, L1 = MAP_SIZE**2, L2 = H_LEV**6, L3 = N_ACTIONS-1)\n",
    "        shipy_pos = (MAP_SIZE**2-1)/2 #shipyard is at the center of the map\n",
    "        if (sp_dec[0] == shipy_pos and s_dec[0] != shipy_pos):\n",
    "            passages = passages +1\n",
    "                \n",
    "        a_temp_enc = ship.greedy_policy(sp_enc, q_values) # simulate the best action in the new state (before update)\n",
    "\n",
    "        # update Q-values\n",
    "        q_values = ship.update_q_v1(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, alpha = LEARNING_RATE,\n",
    "                    gamma = DISCOUNT_FACTOR, map_size = MAP_SIZE, h_lev = H_LEV, n_actions = N_ACTIONS)\n",
    "\n",
    "        # update states and halite\n",
    "        s_enc = sp_enc\n",
    "        current_halite = new_halite\n",
    "\n",
    "        if (finish == True) or (steps >= 400):\n",
    "            if verbose:\n",
    "                print(\"\\nEnd episode.\")\n",
    "            break\n",
    "    collected_halite = current_halite - 4000\n",
    "    return q_values, reward, collected_halite, passages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To skip a passage I already copied this function in the ship.py script in the Modules folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values, reward, collected_halite, passages = ship.play_episode(q_values, verbose = True, **env_dict, **state_dict, **learning_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reward: %.4f \\nCollected halite : %.0f \\nPassages through shipyard: %d\"%(reward, collected_halite, passages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to plot this quantities in real time during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@\n",
    "# Learning variables\n",
    "#@@@@@@@@@@@@@@@@@@@\n",
    "q_values = np.zeros((N_STATES,N_ACTIONS)) #initialize to zero\n",
    "#q_values = np.load(\"Q_values.npy\") # or re-use the one already trained\n",
    "reward_score = np.zeros(MAX_EPOCHS)\n",
    "halite_score = np.zeros(MAX_EPOCHS)\n",
    "shipy_pass = np.zeros(MAX_EPOCHS)\n",
    "epochs = 0\n",
    "eps = EPS_START # starting value of epsilon\n",
    "# generate an adaptive epsilon greedy algorithm, calibrated in order to have epsilon = 10^-4 at the last epoch\n",
    "epsilons = np.array(list(map(lambda i : eps*np.exp(-i*2*np.log(10)/MAX_EPOCHS), np.arange(0,MAX_EPOCHS+1))))\n",
    "\n",
    "\n",
    "# visualize online the results\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "plt.ion()\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.set_xlim(0,MAX_EPOCHS+1)\n",
    "ax1.set_ylim(0,9000)\n",
    "ax1.set_xlabel(\"Epochs\", fontsize = 14)\n",
    "ax1.set_ylabel(\"Halite collected\", fontsize = 14)\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.set_xlim(0,MAX_EPOCHS+1)\n",
    "ax2.set_ylim(-4,5)\n",
    "ax2.set_xlabel(\"Epochs\", fontsize = 14)\n",
    "ax2.set_ylabel(\"Reward\", fontsize = 14)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.set_xlim(0,MAX_EPOCHS+1)\n",
    "ax3.set_ylim(0,0.5)\n",
    "ax3.set_xlabel(\"Epochs\", fontsize = 14)\n",
    "ax3.set_ylabel(\"Epsilon\", fontsize = 14)\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.set_xlim(0,MAX_EPOCHS+1)\n",
    "ax4.set_xlabel(\"Epochs\", fontsize = 14)\n",
    "ax4.set_ylabel(\"Shipyard passages\", fontsize = 14)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "from tqdm import tnrange, trange\n",
    "\n",
    "for k in tnrange(MAX_EPOCHS, desc='1st loop', leave=True):\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    reward_progress = np.zeros(N_BATCH) # bunch of 100 episodes\n",
    "    halite_progress = np.zeros(N_BATCH) # bunch of 100 episodes\n",
    "    shipy_pass_progress = np.zeros(N_BATCH) # bunch of 100 episodes\n",
    "    eps = epsilons[epochs]\n",
    "    # update the dictionary at each epoch with the new epsilon\n",
    "    learning_dict = dict(LEARNING_RATE = 0.1, DISCOUNT_FACTOR = DISCOUNT_FACTOR , eps = eps, STD_REWARD = STD_REWARD)\n",
    "\n",
    "    for i in tnrange(N_BATCH, desc='2nd loop', leave=False):\n",
    "        # here starts an episode\n",
    "        q_values, reward, collected_halite, passages = play_episode(q_values, **env_dict, **state_dict, **learning_dict)\n",
    "        reward_progress[i] = reward\n",
    "        halite_progress[i] = collected_halite\n",
    "        shipy_pass_progress[i] = passages\n",
    "\n",
    "        #break # play just 1 episode\n",
    "\n",
    "    #break # play just 1 epoch\n",
    "\n",
    "    reward_score[epochs-1] = reward_progress.mean()\n",
    "    halite_score[epochs-1] = halite_progress.mean()\n",
    "    shipy_pass[epochs-1] = shipy_pass_progress.mean()\n",
    "    \n",
    "    ax1.scatter(epochs, halite_score[epochs-1], s = 10, color='blue')\n",
    "    ax2.scatter(epochs, reward_score[epochs-1], s = 10,color='blue')\n",
    "    ax3.scatter(epochs, epsilons[epochs], s = 10, color='blue')\n",
    "    ax4.scatter(epochs, shipy_pass[epochs-1], s = 10, color='blue')\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    if epochs >= MAX_EPOCHS:\n",
    "        print(\"Hey, I think you've had enough! Let's stop here.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Rendering\n",
    "\n",
    "In this section we try to render an entire episode for a given state of the Q-values, considering them as definitive. The idea is to color the sea with a blue tonality that becomes brighter as the halite in the cell increases and to color the ship with some brown to yellow tonality, again with the brighter colors for the richest states. Also between one frame and the other, we show in a scale between red and green the Q-values for the adjacent cells, so that we can see why a certain action has been taken.\n",
    "\n",
    "We do not enter into the details of how this visualization is done, but the code is all here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = np.load('Q_values/Q_values.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some functions to represent in RGB the halite levels of the map and the ship\n",
    "\n",
    "def color_sea(map_halite):\n",
    "    #color1 = np.array([32,124,157])\n",
    "    sea_colors = np.zeros((1001,3))\n",
    "    for i in range(1001):\n",
    "        sea_colors[i] = [32,20+i/10,50+2*i/10]\n",
    "    N = len(map_halite.flatten())\n",
    "    rgb_map = np.zeros((N,3))\n",
    "    flatten_map = map_halite.flatten()\n",
    "    for i in range(N):\n",
    "        rgb_map[i] = sea_colors[int(flatten_map[i])]\n",
    "    return rgb_map.reshape((map_halite.shape)+(3,))/255\n",
    "\n",
    "def color_ship(cargo):\n",
    "    #color2 = np.array([255,203,119])\n",
    "    ship_colors = np.zeros((1001,3))\n",
    "    for i in range(1001):\n",
    "        ship_colors[i] = [(120+2*i/10)*250/320,(70+2*i/10)*200/270,(90+i/10)*120/210]\n",
    "    return ship_colors[int(cargo)]/255    \n",
    "\n",
    "def decision_map(q_s, x, y, rgb_map, map_size = 7, alpha=0.5):\n",
    "    \n",
    "    def Q_to_color(q_array):\n",
    "        x = (q_array-q_array.min()+ 1e-5)/(q_array.max() - q_array.min() + 1e-5)\n",
    "        color = np.array([[225*(1-xi),225*xi,25] for xi in x])/255\n",
    "        return color\n",
    "    \n",
    "    pos_dict = {0:(0,0), 1:(1,0), 2:(-1,0), 3:(0,1), 4:(0,-1)}\n",
    "    q_colors = Q_to_color(q_s)\n",
    "    pos_dec = (x,y)\n",
    "    for i in range(5):\n",
    "        xi = (pos_dec[0]+pos_dict[i][0])%map_size\n",
    "        yi = (pos_dec[1]+pos_dict[i][1])%map_size\n",
    "        rgb_map[xi,yi] = alpha*q_colors[i]+(1-alpha)*rgb_map[xi,yi]\n",
    "    return rgb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "\n",
    "env = Env.HaliteEnv(NUM_PLAYERS, MAP_SIZE, episode_lenght = 100) # init environment\n",
    "steps = 0\n",
    "reward = 0 # cumulative reward of the episode\n",
    "passages = 0 # number of times the ship passes through the shipyard\n",
    "verbose = False\n",
    "save = False # saves each frame to make a gif\n",
    "eps = 0\n",
    "# first mandatory step\n",
    "steps = steps + 1\n",
    "if verbose:\n",
    "    print(\"\\nStep number %d:\"%steps)\n",
    "action_matrix = np.full((MAP_SIZE,MAP_SIZE), -1) # no ship, no action\n",
    "shipyard_action = True # initially always choose to create a ship\n",
    "# returns the matricial state, the array of players halite and a flag that is true if it's the final turn\n",
    "state, players_halite, finish, _ = env.step(action_matrix, makeship = shipyard_action) \n",
    "\n",
    "#print(\"Cargo layer: \\n\", state[:,:,2])\n",
    "current_halite = players_halite[0][0]\n",
    "#----------------------------------------------------------\n",
    "rgb_map = color_sea(state[:,:,0]) # get rgb map of blue tonalities\n",
    "plt.imshow(rgb_map) # show map\n",
    "plt.title(\"Turn: %d    Halite: %d\"%(steps,current_halite))\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "fig.show()\n",
    "time.sleep(0.75) #uncomment to slow down for visualization purposes\n",
    "if save:\n",
    "    plt.savefig('.raw_gif/turn%.3d.png'%steps)\n",
    "#-----------------------------------------------------------\n",
    "s_enc = cod.encode_state(state, map_size = MAP_SIZE, h_lev = H_LEV, n_actions = N_ACTIONS, debug=False)\n",
    "\n",
    "while True:\n",
    "    steps = steps + 1\n",
    "    if verbose:\n",
    "        print(\"\\nStep number %d:\"%steps)\n",
    "        print(\"Current halite: \", current_halite)\n",
    "    a_enc = ship.e_greedy_policy(s_enc, q_values, eps = eps)\n",
    "    a_mat = cod.scalar_to_matrix_action(a_enc, state, map_size = MAP_SIZE) #convert the action in matricial form\n",
    "    s_dec = cod.decode3D(s_enc, L1 = MAP_SIZE**2, L2 = H_LEV**6, L3 = N_ACTIONS-1)\n",
    "    x,y = cod.decode(s_dec[0], L=MAP_SIZE) # position of the ship, used in rendering\n",
    "    ship_cargo = state[x,y,2] # halite in the ship, used in rendering\n",
    "    # submit the action and get the new state\n",
    "    state, players_halite, finish, _ = env.step(a_mat, makeship = False) \n",
    "\n",
    "    new_halite = players_halite[0][0]\n",
    "\n",
    "    # compute the 1-ship reward as the halite increment of the player divided by the max halite \n",
    "    # plus a standard negative reward \n",
    "    r = (new_halite - current_halite)/1000 + STD_REWARD\n",
    "\n",
    "    sp_enc = cod.encode_state(state, map_size = MAP_SIZE, h_lev = H_LEV, n_actions = N_ACTIONS, debug=False)\n",
    "    reward += r # cumulative reward of the episode\n",
    "\n",
    "    # adds 1 to passages if the current position of the ship coincides with that of the shipyard\n",
    "    # whereas the previous position didn't\n",
    "    s_dec = cod.decode3D(s_enc, L1 = MAP_SIZE**2, L2 = H_LEV**6, L3 = N_ACTIONS-1)\n",
    "    sp_dec = cod.decode3D(sp_enc, L1 = MAP_SIZE**2, L2 = H_LEV**6, L3 = N_ACTIONS-1)\n",
    "    shipy_pos = (MAP_SIZE**2-1)/2 #shipyard is at the center of the map\n",
    "    if (sp_dec[0] == shipy_pos and s_dec[0] != shipy_pos):\n",
    "        passages = passages +1\n",
    "    \n",
    "    a_temp_enc = ship.greedy_policy(sp_enc, q_values) # simulate the best action in the new state (before update)\n",
    "    \n",
    "    #----------------------------------------------------------\n",
    "    rgb_map = color_sea(state[:,:,0]) # get rgb map of blue tonalities\n",
    "    rgb_map[x ,y,:] = color_ship(ship_cargo) # superimpose the ship pixel with a yellow tonality\n",
    "    plt.cla() # clear current axis from previous drawings -> prevents matplotlib from slowing down\n",
    "    plt.imshow(rgb_map)\n",
    "    plt.title(\"Turn: %d    Halite: %d    Carried: %d\"%(steps,current_halite,ship_cargo))\n",
    "    plt.yticks([]) # remove y ticks\n",
    "    plt.xticks([]) # remove x ticks\n",
    "    fig.canvas.draw() # update the figure\n",
    "    time.sleep(0.75) #uncomment to slow down for visualization purposes\n",
    "    if save:\n",
    "        plt.savefig('.raw_gif/turn%.3d.png'%(steps*2))\n",
    "    # shows Q-values projected on the cells adjacent to the ship\n",
    "    rgb_map = decision_map(q_values[s_enc], x, y, rgb_map, MAP_SIZE) \n",
    "    plt.imshow(rgb_map)\n",
    "    #move_dict = {0:'C',1:'S',2:'N',3:'E',4:'W'}\n",
    "    plt.title(\"Turn: %d    Halite: %d    Carried: %d\"%(steps, current_halite, ship_cargo))\n",
    "    #+\"Next action: \"+move_dict[a_enc]) # to print also what the next move will be\n",
    "    plt.yticks([]) # remove y ticks\n",
    "    plt.xticks([]) # remove x ticks\n",
    "    fig.canvas.draw() # update the figure\n",
    "    time.sleep(0.75) #uncomment to slow down for visualization purposes\n",
    "    if save:\n",
    "        plt.savefig('.raw_gif/turn%.3d.png'%(steps*2+1))\n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "    # update Q-values\n",
    "    q_values = ship.update_q_v1(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, alpha = LEARNING_RATE,\n",
    "                gamma = DISCOUNT_FACTOR, map_size = MAP_SIZE, h_lev = H_LEV, n_actions = N_ACTIONS)\n",
    "\n",
    "    # update states and halite\n",
    "    s_enc = sp_enc\n",
    "    current_halite = new_halite\n",
    "    if (finish == True) or (steps >= 400):\n",
    "        if verbose:\n",
    "            print(\"\\nEnd episode.\")\n",
    "        break\n",
    "plt.close() # close interactive plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a gif of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filenames = os.listdir('.raw_gif') # get the names of all the files in .raw_gif directory\n",
    "filenames.sort() # sort them by name (i.e. by turn in our specific case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install imageio\n",
    "import imageio\n",
    "images = []\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread('.raw_gif/'+filename))\n",
    "imageio.mimsave('Support_material/play_episode.gif', images, duration=0.75) # make gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Post-training visualization\n",
    "\n",
    "In this section we are going to do some exploratory analysis of the trained Q-values. Hopefully we will get some useful insights on how to improve our training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = np.load(\"Q_values/Q_values.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-training issue\n",
    "\n",
    "First of all we look at the overall distribution of the Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_flat = q_values.flatten()\n",
    "q_nonzero = q_flat[q_flat!=0]\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "n, bins, _ = plt.hist(q_flat, bins=100)\n",
    "plt.title(\"Qv distribution - %d elements\"%len(q_flat), fontsize=15)\n",
    "plt.xlabel(\"Q values\", fontsize=16)\n",
    "plt.ylabel(\"Counts\", fontsize=16)\n",
    "\n",
    "plt.subplot(122)\n",
    "n, bins, _ = plt.hist(q_nonzero, bins=100)\n",
    "plt.title(\"Non-zero Qv distribution - %d elements\"%len(q_nonzero), fontsize=15)\n",
    "plt.xlabel(\"Q values\", fontsize=16)\n",
    "plt.ylabel(\"Counts\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the particular Q-values that we are using are the result of a training of 200 epochs of 10 batches of 400 turns each, for a total of 800.000 moves, but it turns out that they explored only 45848 different action-state pairs (some elements were probably different berofe quantization, but this is not the point). \n",
    "We can define a measure, that we call novelty, as the ratio the states (we call them states instead of action-state pairs for a shorter notatio) explored and the moves made:\n",
    "$$novelty = \\frac{non~zero~ states}{tot. moves} = 0.0573 = 5.73 \\%$$\n",
    "\n",
    "A naive approach would suggest that we should train for the total number of states divided by the novelty factor to explore all the states, but of course the novelty is not a constant, but it is roughly a decreasing function of the number of moves that we have already done (e.g. for the first move the novelty is 1). \n",
    "\n",
    "For sure this means that more training will be useful, but it is also true that as the novelty decreases also the probability of making an error because we do not know how to evaluate a move. In general with such a high-dimensional space the only way to have an estimate of that probability is to measure it during training and then interpolate the function to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State values maps\n",
    "\n",
    "Another thing that we can try to visualize is how good a spatial position is, given that we have a certain amount of cargo on board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = q_values.max(axis=1) # take the best possible action given the state -> state value\n",
    "HD_shape = (MAP_SIZE**2,)+tuple([3 for i in range(6)]) + (4,)\n",
    "print(\"High-dimensional shape of the state values: \", HD_shape)\n",
    "values_HD = values.reshape(HD_shape)\n",
    "# now spit the cargo dimension in 3 different sub-arrays\n",
    "low_cargo = values_HD[:,0,...]\n",
    "medium_cargo = values_HD[:,1,...]\n",
    "high_cargo = values_HD[:,2,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cargo_max = low_cargo.reshape(49,-1).max(axis=1).reshape(7,7)\n",
    "low_cargo_min = low_cargo.reshape(49,-1).min(axis=1).reshape(7,7)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(low_cargo_max)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Best state values for low cargo\", fontsize=16)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(low_cargo_min)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Worst state values for low cargo\", fontsize=16)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "medium_cargo_max = medium_cargo.reshape(49,-1).max(axis=1).reshape(7,7)\n",
    "medium_cargo_min = medium_cargo.reshape(49,-1).min(axis=1).reshape(7,7)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(medium_cargo_max)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Best state values for medium cargo\", fontsize=16)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(medium_cargo_min)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Worst state values for medium cargo\", fontsize=16)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "high_cargo_max = high_cargo.reshape(49,-1).max(axis=1).reshape(7,7)\n",
    "high_cargo_min = high_cargo.reshape(49,-1).min(axis=1).reshape(7,7)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(high_cargo_max)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Best state values for high cargo\", fontsize=16)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(high_cargo_min)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Worst state values for high cargo\", fontsize=16)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots contain a lot of information:\n",
    "1. Best states for each level of cargo are learned in a more symmetric way than the worse ones; this is because they are choosen more times, hence their estimates are more reliable.\n",
    "2. For medium and high levels of cargo the shipyard position has value zero, whereas for the low level it is greater than zero. This is because the ship can never have cargo when is in the shipyard position (it is automatically deposited). As we have seen with the shipyard passages metric, this is not a problem, because the ship just needs to learn that the action of going to the shipyard is good, and this is confirmed by the fact that the highest values are in the proximity of the shipyard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is Halite direction needed?\n",
    "\n",
    "If you remember the `h_dir` variable encodes the direction in which there is the 3x3 area with the higher amount of halite. We would like to know if this is a good indicator to predict the best action to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cardinals = q_values[:,1:] # remove a=0 subset of moves\n",
    "HD_shape2 = (MAP_SIZE**2,)+tuple([3 for i in range(6)]) + (4,4)\n",
    "q_HD = q_cardinals.reshape(HD_shape2)\n",
    "plt.figure(figsize=(10,8))\n",
    "for i in range(4):\n",
    "    q_hdir = q_HD[...,i].reshape(-1,4) # select h_dir = i, then reshape as a matrix (s',a)\n",
    "    #nonzero = np.count_nonzero(q_hdir0, axis=0)\n",
    "    #q_mean0 = q_hdir0.sum(axis=0)/nonzero # mean over non-zero elements\n",
    "    # compute standard deviation of non zero elements\n",
    "    q_mean = []\n",
    "    q_std = []\n",
    "    for j in range(4):\n",
    "        x = q_hdir[:,j]\n",
    "        x_filtered = x[x!=0]\n",
    "        mean = x_filtered.mean()\n",
    "        std = x_filtered.std()/np.sqrt(len(x_filtered)) #deviation of the mean\n",
    "        q_mean.append(mean)\n",
    "        q_std.append(std)\n",
    "    plt.errorbar(np.arange(4), q_mean, yerr = q_std, label='h_dir = '+str(i))\n",
    "plt.legend()\n",
    "plt.xticks([0,1,2,3])\n",
    "plt.title(\"Mean action-value vs h_dir\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is a correlation between the mean value of a given action for different halite directions, but it is quite puzzling the fact that in general it is negatively correlated with the mean Q-value (that is, acting accordingly to the h_dir results in worse Q-values on average). A possible explanation is that usually the richest direction is never the one towards the shipyard, whereas the highest Q-values are always obtained moving towards the shipyard. \n",
    "\n",
    "To see more accurately if h_dir is useful, we should look at the typical situation for which that information was encoded, that is when the ship needs to collect resources (hence low cargo) but all around it there is low halite.\n",
    "This could be typical of the final turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cardinals = q_values[:,1:] # remove a=0 subset of moves\n",
    "HD_shape2 = (MAP_SIZE**2,)+tuple([3 for i in range(6)]) + (4,4)\n",
    "q_HD = q_cardinals.reshape(HD_shape2)\n",
    "plt.figure(figsize=(10,8))\n",
    "for i in range(4):\n",
    "    q_hdir0 = q_HD[:,0,0,0,0,0,0,i].reshape(-1,4) # select cargo=0, h_dir = i, then reshape as a matrix (s',a)\n",
    "    q_mean = []\n",
    "    q_std = []\n",
    "    for j in range(4):\n",
    "        x = q_hdir0[:,j]\n",
    "        x_filtered = x[x!=0]\n",
    "        mean = x_filtered.mean()\n",
    "        std = x_filtered.std()/np.sqrt(len(x_filtered)) #deviation of the mean\n",
    "        q_mean.append(mean)\n",
    "        q_std.append(std)\n",
    "    plt.errorbar(np.arange(4), q_mean, yerr = q_std, label='h_dir = '+str(i))\n",
    "plt.legend()\n",
    "plt.xticks([0,1,2,3])\n",
    "plt.title(\"Mean action-value vs h_dir - no halite in proximity\", fontsize=16)\n",
    "plt.xlabel(\"Action taken\", fontsize=16)\n",
    "plt.ylabel(\"Mean Q-value for that action\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can actually see that, all other things being equal, it is on average convenient to move towards the direction with the richest halite (for each curve the maximum is at the direction corresponding to the halite direction).\n",
    "\n",
    "At the same time we can see that h_dir is completely ininfluent if we take all adjacent cells to zero except one direction. For example in the next plot we search for states that have the cell south to the ship with a higher value than the other ones and we find a peak for the action that goes toward the richest cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cardinals = q_values[:,1:] # remove a=0 subset of moves\n",
    "HD_shape2 = (MAP_SIZE**2,)+tuple([3 for i in range(6)]) + (4,4)\n",
    "q_HD = q_cardinals.reshape(HD_shape2)\n",
    "plt.figure(figsize=(10,8))\n",
    "for i in range(4):\n",
    "    q_hdir0 = q_HD[:,0,0,1,0,0,0,i].reshape(-1,4) # select cargo=0, h_dir = i, then reshape as a matrix (s',a)\n",
    "    q_mean = []\n",
    "    q_std = []\n",
    "    for j in range(4):\n",
    "        x = q_hdir0[:,j]\n",
    "        x_filtered = x[x!=0]\n",
    "        mean = x_filtered.mean()\n",
    "        std = x_filtered.std()/np.sqrt(len(x_filtered)) #deviation of the mean\n",
    "        q_mean.append(mean)\n",
    "        q_std.append(std)\n",
    "    plt.errorbar(np.arange(4), q_mean, yerr = q_std, label='h_dir = '+str(i))\n",
    "plt.legend()\n",
    "plt.xticks([0,1,2,3])\n",
    "plt.title(\"Peak for the action towards the richest cell \", fontsize=16)\n",
    "plt.xlabel(\"Action taken\", fontsize=16)\n",
    "plt.ylabel(\"Mean Q-value for that action\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking home messages\n",
    "\n",
    "In this tutorial we have seen three different methods to visualize what our agent is learning. \n",
    "\n",
    "* The first one was to show some average metrics performances during training, so that we can see how the training is going.\n",
    "\n",
    "* The second one is to render an episode in a videogame style, that is the most intuitive visualization and can also support some meta-information encoding through colors and writings.\n",
    "\n",
    "* The third one is to unpack the Q-values matrix in order to recover its high dimensional version and then select particular conditions that can be thought as hyperplanes untill we reduce the dimension enough to be visualized.\n",
    "\n",
    "In the next tutorial we will address what by now you should have noticed as the major problem of the agent trained so far, that is that its performance is suboptimal. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_halite",
   "language": "python",
   "name": "python3_halite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
