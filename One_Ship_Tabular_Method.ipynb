{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halite challenge - basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Environment import halite_env as Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Environment.halite_env' from '/home/nicola/Nicola_unipd/QuartoAnno/TODO/Baiesi/RL/haliteRL/Environment/halite_env.py'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(Env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the environment - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_players = 1\n",
    "map_size = 7 # 7 x 7 map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Halite Environment\n"
     ]
    }
   ],
   "source": [
    "env = Env.HaliteEnv(num_players, map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[913, 885, 307,  85, 278, 325, 175],\n",
       "       [240, 284, 143, 362, 170, 929, 404],\n",
       "       [734, 119, 458, 502, 600, 383, 368],\n",
       "       [132, 520, 298,   0, 191, 397, 634],\n",
       "       [635, 334, 109, 280, 769, 820,  39],\n",
       "       [646, 764, 704,  25, 775, 984, 621],\n",
       "       [161, 186, 290, 314, 652, 112, 654]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# halite in the map, min = 0, max = 1000\n",
    "env.map[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ship position\n",
    "env.map[:,:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the halite carried from each ship in the position corresponding to the ship\n",
    "# initially there is no ship, hence no halite carried either\n",
    "env.map[:,:,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shipyard position\n",
    "env.map[:,:,3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial halite:  [5000.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial halite: \", env.player_halite[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000.]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# actions are represented as a matrix whose entries are -1 if no ship is in that position, \n",
    "#'a_i' if ship i is present in that position \n",
    "action_matrix = np.full((map_size,map_size), -1) # no ship, no action\n",
    "\n",
    "# the environment already has in memory the last state, thus we don't need to resubmit it\n",
    "# the only things that we submit are the action matrix and the shipyard action (1 or True to spawn a ship, 0 otherwise)\n",
    "shipyard_action = 1 # initially always choose to create a ship\n",
    "# returns the state, i.e. env.map\n",
    "s, h, finish, _ = env.step(action_matrix, makeship = shipyard_action)\n",
    "print(h[0])\n",
    "print(finish)\n",
    "# s_0 -> map_halite, s_1 -> ship_position, s_2 -> cargo_halite, s_3 -> shipyard_position (not used)\n",
    "map_halite = s[:,:,0]\n",
    "ship_pos_matrix = s[:,:,1]\n",
    "shipy_pos_matrix = s[:,:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State complexity and state approximation\n",
    "\n",
    "We have:\n",
    "- $(map\\_size)^2$ positions ($49$ in this case, up to $64^2 = 4096$ for the largest map);\n",
    "- $1000$ values of halite for each position ($h\\_lev$);\n",
    "- $1000$ values of carried halite ($h\\_lev$).\n",
    "\n",
    "The state of the system is defined by the position of the ship + the halite in EACH cell + the halite carried by the ship. We can have almost all the possible combinations of the values that those variables can assume, thus we have the combinations of $1000$ values of halite for $50$ cells ($49$ of the map + the one carried by the ship) all multiplied by $49$ possible positions of the ship, for a total of $49\\times 10^{147}$ possible states. If instead we consider the largest map of $64 \\times 64$ we arrive at $4096\\times 10^{12288}$ possible states.\n",
    "\n",
    "The general formula can be written as: \n",
    "$$\\# states = (map\\_size)^2 \\times (h\\_lev)^{map\\_size^2+1}$$\n",
    "\n",
    "where we consider the map always centered on the shipyard, hence in this framework its position is fixed and not considered as a variable.\n",
    "Of course this state representation is totally uncontrollable, because it scales exponentially in the number of cells of the map.\n",
    "\n",
    "To tackle this issue we choose to drastically reduce the amount of information that is observed by the ship through two processes: partial observation and state abstraction.\n",
    "\n",
    "\n",
    "\n",
    "### Partial observation: depth of field\n",
    "\n",
    "The most expensive dependence in the formula about the number of states of the system is the exponent at which is elevated $h\\_lev$. This is obtained considering all possible combinations of halite for all the cells of the map and the halite carried by the ship. A different approach is to consider only the halite inside the field of view of the agent and restrict the depth of field to the minimal possible quantity, i.e. nearest neighbors. In this way, independently from the $map\\_size$ we get an exponent that in 2D is equal to 6 (4 for the neighbors, 1 for the state in which the ship stands and 1 for the halite it carries).\n",
    "\n",
    "In other words, we get: \n",
    "\n",
    "$$\\# states = (map\\_size)^2 \\times (h\\_lev)^{6}$$\n",
    "\n",
    "that yields $4.9 \\times 10^{19}$ for the $7 \\times 7$ map, that is still not manageable, but considerably smaller ( of order $ \\approx 10^{120}$).\n",
    "\n",
    "### State abstraction: halite quantization\n",
    "\n",
    "Differently from the restriction on the observation space, that are somewhat straightforward, the state abstraction must involve some hypothesis about the environment that involve knowing the model of the environment. For example, if we were in the situation on not knowing how the halite is collected we probably would have done a different choice.\n",
    "Since we know that the ship collects an amount of halite proportional to the halite in the cell (25% of it, to be more exact) and pays a fee of 10% of the halite contained in a cell to leave it, we are more interested in having encoded the notions of \"low\" and \"high\" halite levels, instead of sampling with precision the middle-high half of the halite scale. \n",
    "To be more specific, we choose to approximate the information about the halite using for $h_lev = 3$ halite levels and the following encoding:\n",
    "- $h = 0$ if $halite \\le 10$; \n",
    "- $h = 1$ if $10 < halite \\le 100$; \n",
    "- $h = 2$ if $100 < halite \\le 1000$.\n",
    "\n",
    "The important part is that the halite is quantized in intervals that grow of a decade each, but we could also test adding a fourth level.\n",
    "In this way the number of states of the system becomes:\n",
    "\n",
    "$$\\# states = (map\\_size)^2 \\times 3^{6}$$\n",
    "\n",
    "yielding for a $7 \\times 7$ map $35.721$ states, that is reachable with our resources. \n",
    "\n",
    "### State abstraction: meta-informations\n",
    "\n",
    "The problem of these manipulations is that now the ship has access only to local informations and lacks of the knowledge about the position of the shipyard (but again, being the latter fixed, only the ship position is needed) and of that of distant halite deposits. In order to enhance the ability of the ship to find those deposits, we encode in a 4-states additional information the direction that the ship should take to go towards the nearest and richest deposit.\n",
    "In this final formulation, the total number of states that needs to be experienced by the ship is:\n",
    "\n",
    "$$\\# states = (map\\_size)^2 \\times 3^{6} * 4$$\n",
    "\n",
    "yielding for a $7 \\times 7$ map the final result of $142.884$ states. Considering that each of these states requires 64 bits, i.e. 8 bytes, to be stored, the memory required to store the Q-value table is 1.143.072 bytes = 1.143 Mb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State encoding\n",
    "\n",
    "We have seen how many states we have to handle for a $7 \\times 7$ map, but each state is composed by different informations:\n",
    "\n",
    "- $pos\\_enc \\in [0,48]$ (ship position encoded);\n",
    "- $halite\\_vector = (C, O, S, N, E, W)$ halite (where C stands for the halite carried by the ship and O for the cell occupied by the ship );\n",
    "- $halite\\_direction \\in [1,4]$ (action to take to go towards the nearest and richest halite deposit).\n",
    "\n",
    "The idea is to first encode the vector of halite using a one hot encoding, then to form a 3D tensor of $pos\\_enc \\times halvec\\_enc \\times haldir$ and encode it in a 1D array. This final array $s\\_enc$ will form the rows of the Q(s,a) table and should assume values between 0 and 142.883."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Halite Environment\n"
     ]
    }
   ],
   "source": [
    "# reinitialize the environment\n",
    "env = Env.HaliteEnv(num_players, map_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_matrix = np.full((map_size,map_size), -1) # no ship, no action\n",
    "shipyard_action = 1 # initially always choose to create a ship\n",
    "# returns the state, i.e. env.map\n",
    "s, h, finish, _ = env.step(action_matrix, makeship = shipyard_action)\n",
    "# s_0 -> map_halite, s_1 -> ship_position, s_2 -> cargo_halite, s_3 -> shipyard_position (not used)\n",
    "map_halite = s[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ship position (encoded and decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix to scalar encoding\n",
    "\n",
    "def one_to_index(V,L):\n",
    "    # matrix V with one entry = 1 and the others 0\n",
    "    return np.arange(L**2).reshape((L, L))[V.astype(bool)]\n",
    "\n",
    "# 2D encoding and decoding\n",
    "\n",
    "def encode(v_dec, L):\n",
    "    # v_two = [v1,v2]\n",
    "    # returns the encoded version V[v1,v2] of V = np.arange(0,L)\n",
    "    # L = length(all_possible_v)\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_enc = V[v_dec[0],v_dec[1]] \n",
    "    return v_enc\n",
    "\n",
    "def decode(v_enc, L):\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0]])\n",
    "    return v_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded position of the ship:  [24]\n",
      "Decoded position of the ship:  [3 3]\n",
      "Encoded position of the shipyard:  [24]\n",
      "Decoded position of the shipyard:  [3 3]\n"
     ]
    }
   ],
   "source": [
    "ship_pos_matrix = s[:,:,1]\n",
    "shipy_pos_matrix = s[:,:,3]\n",
    "\n",
    "#position_encoded of the ship\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size)\n",
    "print(\"Encoded position of the ship: \", pos_enc)\n",
    "#position_decoded of the ship\n",
    "pos_dec = decode(pos_enc, map_size)\n",
    "print(\"Decoded position of the ship: \", pos_dec)\n",
    "\n",
    "#position_encoded of the ship\n",
    "shipy_enc = one_to_index(shipy_pos_matrix, map_size)\n",
    "print(\"Encoded position of the shipyard: \", shipy_enc)\n",
    "#position_decoded of the ship\n",
    "shipy_dec = decode(shipy_enc, map_size)\n",
    "print(\"Decoded position of the shipyard: \", shipy_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting halite vector (encoded and decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halite_vec_dec(state, q_number = 3, map_size = 7):\n",
    "    \n",
    "    def halite_quantization(h, q_number = 3):\n",
    "        # h can either be a scalar or a matrix \n",
    "        tresholds = np.logspace(1,3,q_number) # [10, 100, 1000] = [10^1, 10^2, 10^3]\n",
    "        h_shape = h.shape\n",
    "        h_temp = h.flatten()\n",
    "        mask = (h_temp[:,np.newaxis] < tresholds).astype(int)\n",
    "        level = np.argmax(mask, axis = 1)\n",
    "        return level.reshape(h_shape)\n",
    "\n",
    "    pos_enc = one_to_index(state[:,:,1], map_size)\n",
    "    pos_dec = decode(pos_enc, map_size) # decode position to access matrix by two indices\n",
    "    \n",
    "    ship_cargo = state[pos_dec[0],pos_dec[1],2]\n",
    "    #print(\"Halite carried: \", ship_cargo)\n",
    "    cargo_quant = halite_quantization(ship_cargo).reshape(1)[0] # quantize halite\n",
    "    \n",
    "    map_halite = state[:,:,0]\n",
    "    halite_quant = halite_quantization(map_halite) # quantize halite\n",
    "    \n",
    "    halite_vector = []\n",
    "    halite_vector.append(cargo_quant)\n",
    "    halite_vector.append(halite_quant[pos_dec[0], pos_dec[1]])\n",
    "    halite_vector.append(halite_quant[(pos_dec[0]+1)%map_size, pos_dec[1]])\n",
    "    halite_vector.append(halite_quant[(pos_dec[0]-1)%map_size, pos_dec[1]])\n",
    "    halite_vector.append(halite_quant[pos_dec[0], (pos_dec[1]+1)%map_size])\n",
    "    halite_vector.append(halite_quant[pos_dec[0], (pos_dec[1]-1)%map_size])\n",
    "    #print(\"Quantized halite vector: \", halite_vector)\n",
    "    return np.array(halite_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "halvec_dec = get_halite_vec_dec(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to encode this vector of 6 elements whose values can be 0,1 or 2 in a scalar between 0 and $3^6 - 1$ (the encoding, of course, must be unique). The idea is similar to that of the 2D case, where we have a matrix whose element are numbered from 0 to the total number of elements - 1:\n",
    "- to get the encoded state we read the element value whose indices are given by the ordered values stored in the array;\n",
    "- to decode the encoded state and recover the decompressed array, we create a tensor (before it was a matrix) with the same shape of the tensor (matrix) that contains all the numbered states; this tensor has all the entries equal to the scalar value of the encoded state and we compare the two tensor and take the indices of the only element that is equal to the encoded state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid for encoding and decoding an array of length L whose entries can all assume only \n",
    "# the same integer values from 0 to m\n",
    "def encode_tensor(v_dec, L = 6, m = 3):\n",
    "    T = np.arange(m**L).reshape(tuple([m for i in range(L)]))\n",
    "    return T[tuple(v_dec)]\n",
    "\n",
    "def decode_tensor(v_enc, L = 6, m = 3):\n",
    "    T = np.arange(m**L).reshape(tuple([m for i in range(L)]))\n",
    "    return np.array([np.where(v_enc == T)[i][0] for i in range(L)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vector:  [0 0 2 2 2 2]\n",
      "Encoded vector:  80\n",
      "Decoded vector:  [0 0 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original vector: \", halvec_dec)\n",
    "v_enc = encode_tensor(halvec_dec)\n",
    "print(\"Encoded vector: \", v_enc)\n",
    "v_dec = decode_tensor(v_enc)\n",
    "print(\"Decoded vector: \", v_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting halite direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halite_direction(state, map_size = 7):\n",
    "    \n",
    "    def roll_and_cut(M, shift, axis, border = 1, center = (3,3)):\n",
    "        M_temp = np.roll(M, shift = shift, axis = axis)\n",
    "        M_cut = M_temp[center[0]-border:center[0]+border+1, center[1]-border:center[1]+border+1]\n",
    "        return M_cut\n",
    "\n",
    "    map_halite = state[:,:,0] # matrix with halite of each cell of the map\n",
    "    \n",
    "    pos_enc = one_to_index(state[:,:,1], map_size) # ship position\n",
    "    pos_dec = decode(pos_enc, map_size) # decode position to access matrix by two indices\n",
    "    \n",
    "    shipy_enc = one_to_index(shipy_pos_matrix, map_size) # shipyard position\n",
    "    shipy_dec = decode(shipy_enc, map_size) #position_decoded \n",
    "    \n",
    "    shift = (shipy_dec[0]-pos_dec[0],shipy_dec[1]-pos_dec[1])\n",
    "    centered_h = np.roll(map_halite, shift = shift, axis = (0,1)) #centers map_halite on the ship\n",
    "    \n",
    "    mean_cardinal_h = []\n",
    "    # this could be generalized to wider areas, like 5x5, but 3x3 it's enough for a 7x7 map\n",
    "    perm = [(a,sh) for a in [0,1] for sh in [-2,2]] # permutations of shifts and axis to get the 4 cardinal directions\n",
    "    for a,sh in perm:\n",
    "        mean_h = np.mean(roll_and_cut(centered_h, shift = sh, axis = a), axis = (0,1))\n",
    "        mean_cardinal_h.append(mean_h)\n",
    "\n",
    "    mean_cardinal_h = np.array(mean_cardinal_h)\n",
    "    halite_direction = np.argmax(mean_cardinal_h) #+ 1 # take the direction of the 3x3 most rich zone\n",
    "    \n",
    "    return halite_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what we should get: \n",
      " [[754 266 965  59 284  59 825]\n",
      " [452 882 536 380 751 725 997]\n",
      " [717 358 764 743 631 950 395]\n",
      " [197 878 811 983 243 745 380]\n",
      " [986  80 771 754   0 696 861]\n",
      " [298 769 724 591 450 584 352]\n",
      " [136 826 870 912 196  69 316]]\n",
      "Result: \n",
      " [[754 266 965  59 284  59 825]\n",
      " [452 882 536 380 751 725 997]\n",
      " [717 358 764 743 631 950 395]\n",
      " [197 878 811 983 243 745 380]\n",
      " [986  80 771 754   0 696 861]\n",
      " [298 769 724 591 450 584 352]\n",
      " [136 826 870 912 196  69 316]]\n",
      "Neighborhood of the ship: \n",
      " [[764 743 631]\n",
      " [811 983 243]\n",
      " [771 754   0]] \n",
      "\n",
      "Map shifted in direction: (-2,0)\n",
      " [[771 754   0]\n",
      " [724 591 450]\n",
      " [870 912 196]]\n",
      "Mean halite in direction: (-2,0) 585.3333333333334 \n",
      "\n",
      "Map shifted in direction: (2,0)\n",
      " [[965  59 284]\n",
      " [536 380 751]\n",
      " [764 743 631]]\n",
      "Mean halite in direction: (2,0) 568.1111111111111 \n",
      "\n",
      "Map shifted in direction: (-2,1)\n",
      " [[631 950 395]\n",
      " [243 745 380]\n",
      " [  0 696 861]]\n",
      "Mean halite in direction: (-2,1) 544.5555555555555 \n",
      "\n",
      "Map shifted in direction: (2,1)\n",
      " [[717 358 764]\n",
      " [197 878 811]\n",
      " [986  80 771]]\n",
      "Mean halite in direction: (2,1) 618.0 \n",
      "\n",
      "Action suggested to reach the nearest and richest halite deposit:  3\n"
     ]
    }
   ],
   "source": [
    "# Explanatory cell\n",
    "\n",
    "# now suppose that the ship is in [2,2], whereas the shipyard is at the center of the map, i.e. [3,3]\n",
    "example = np.roll(map_halite, shift = (1,1) , axis =  (0,1)) #in this way we simulate the ship to be in (2,2)\n",
    "print(\"This is what we should get: \\n\", example)\n",
    "pos_dec = [2,2]\n",
    "shift = (shipy_dec[0]-pos_dec[0],shipy_dec[1]-pos_dec[1])\n",
    "centered_h = np.roll(map_halite, shift = shift, axis = (0,1))\n",
    "print(\"Result: \\n\",centered_h)\n",
    "\n",
    "def roll_and_cut_v0(M, shift, axis, border = 1, center = (3,3)):\n",
    "        M_temp = np.roll(M, shift = shift, axis = axis)\n",
    "        M_cut = M_temp[center[0]-border:center[0]+border+1, center[1]-border:center[1]+border+1]\n",
    "        return M_cut\n",
    "    \n",
    "# try to return just the 3x3 area around the ship\n",
    "around_ship = roll_and_cut_v0(centered_h, shift = 0, axis = 0)\n",
    "print(\"Neighborhood of the ship: \\n\", around_ship, '\\n')\n",
    "# we actually need to do this shifting by two in all cardinal directions w.r.t. the map centered around the ship\n",
    "mean_cardinal_h = []\n",
    "perm = [(a,sh) for a in [0,1] for sh in [-2,2]]\n",
    "for a,sh in perm:\n",
    "    print(\"Map shifted in direction: (%d,%d)\\n\"%(sh,a), roll_and_cut_v0(centered_h, shift = sh, axis = a))\n",
    "    mean_h = np.mean(roll_and_cut_v0(centered_h, shift = sh, axis = a), axis = (0,1))\n",
    "    print(\"Mean halite in direction: (%d,%d)\"%(sh,a), mean_h, '\\n')\n",
    "    mean_cardinal_h.append(mean_h)\n",
    "\n",
    "mean_cardinal_h = np.array(mean_cardinal_h)\n",
    "halite_direction = np.argmax(mean_cardinal_h) #+ 1\n",
    "print(\"Action suggested to reach the nearest and richest halite deposit: \", halite_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and decoding the total state\n",
    "\n",
    "Starting from the encoded position, the encoded halite vector and the halite direction, we finally encode and decode this 3 contributions in a unique scalar that will take values between 0 and 142.883."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D encoding and decoding for arbitrary lengths of the three axis\n",
    "\n",
    "def encode3D(v_dec, L1, L2, L3):\n",
    "    # v_dec = [v1,v2,v3]\n",
    "    # returns the encoded version V[v1,v2,v3] of V = np.arange(0,L1*L2*L3)\n",
    "    #print(\"v_dec: \", v_dec)\n",
    "    #print(\"L1 = %d, L2 = %d, L3 = %d\"%(L1,L2,L3))\n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_enc = V[tuple(v_dec)] \n",
    "    return v_enc\n",
    "\n",
    "def decode3D(v_enc, L1, L2, L3):\n",
    "    # v_enc = V[v1,v2,v3] \n",
    "    # V = np.arange(0,L1*L2*L3)\n",
    "    # returns the decoded version v_dec = [v1,v2,v3] of V[v1,v2,v3] \n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0], np.where(v_enc == V)[2][0]])\n",
    "    return v_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "80\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(pos_enc[0])\n",
    "halvec_enc = encode_tensor(halvec_dec)\n",
    "print(halvec_enc)\n",
    "print(halite_direction)\n",
    "s_dec = np.array([pos_enc[0], halvec_enc, halite_direction] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of states to be experienced:  142884\n",
      "Original decoded state:  [24 80  3]\n",
      "v_dec:  [24 80  3]\n",
      "L1 = 49, L2 = 729, L3 = 4\n",
      "Encoded state:  70307\n",
      "New decoded state:  [24 80  3]\n"
     ]
    }
   ],
   "source": [
    "h_lev = 3 # halite levels\n",
    "n_cells = map_size**2 # number of cells in a square map\n",
    "n_states = n_cells*h_lev**6*4\n",
    "n_actions = 5 # no dropoffs, 1 action for staying still, 4 for moving in the cardinal directions\n",
    "print(\"Total number of states to be experienced: \", n_states)\n",
    "\n",
    "print(\"Original decoded state: \", s_dec)\n",
    "s_enc = encode3D(s_dec, L1 = n_cells, L2 = h_lev**6, L3 = n_actions-1)\n",
    "print(\"Encoded state: \", s_enc)\n",
    "s_dec_2 = decode3D(s_enc, L1 = n_cells, L2 = h_lev**6, L3 = n_actions-1)\n",
    "print(\"New decoded state: \", s_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put all together in order to have a single function that takes the multi-layer state output by the environment and returns the encoded state (that is the one we will use to access the Q-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(state, map_size = 7, h_lev = h_lev, n_actions = n_actions, debug = False):\n",
    "    \n",
    "    pos_enc = one_to_index(state[:,:,1], map_size)[0] # ship position\n",
    "    if debug:\n",
    "        print(\"Ship position encoded in [0,%d]: \"%(map_size**2-1), pos_enc)\n",
    "    \n",
    "    halvec_dec = get_halite_vec_dec(state, q_number = 3, map_size = map_size) \n",
    "    halvec_enc = encode_tensor(halvec_dec) # halite vector\n",
    "    if debug:\n",
    "        print(\"Halite vector encoded in [0,%d]: \"%(h_lev**6 -1), halvec_enc)\n",
    "    \n",
    "    haldir = get_halite_direction(state, map_size = map_size) # halite direction\n",
    "    if debug:\n",
    "        print(\"Halite direction in [1,4]: \", haldir)\n",
    "    \n",
    "    s_dec = np.array([pos_enc, halvec_enc, haldir])\n",
    "    if debug:\n",
    "        print(\"Decoded state: \", s_dec)\n",
    "    s_enc = encode3D(s_dec, L1 = map_size**2, L2 = h_lev**6, L3 = n_actions-1)\n",
    "    if debug:\n",
    "        print(\"State encoded in [0, %d]: \"%(map_size**2*h_lev**6*(n_actions-1)), s_enc, '\\n')\n",
    "    \n",
    "    return s_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded state:  [24 80  3]\n",
      "v_dec:  [24 80  3]\n",
      "L1 = 49, L2 = 729, L3 = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70307"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_state(s, map_size = 7, h_lev = h_lev, n_actions = n_actions, debug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar and matricial action\n",
    "\n",
    "The environment works for a generic number of ships. For this reason the action submitted must be a matrix of $map\\_size \\times map\\_size$ filled with -1 except for the entry of the ship position, where it has to be entered the value of the action choosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_to_matrix_action(action, state, map_size = 7):\n",
    "    # first get the decoded position of the ship\n",
    "    ship_pos_matrix = state[:,:,1]\n",
    "    pos_enc = one_to_index(ship_pos_matrix, map_size)\n",
    "    pos_dec = decode(pos_enc, map_size)\n",
    "    # then fill a matrix of -1\n",
    "    mat_action = np.full((map_size,map_size), -1)\n",
    "    # finally insert the action in the pos_dec entry\n",
    "    mat_action[tuple(pos_dec)] = action\n",
    "    return mat_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@\n",
    "# RL agent functions\n",
    "#@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "def greedy_policy(s, q_values):\n",
    "    return np.argmax(q_values[s])\n",
    "\n",
    "def e_greedy_policy(s, q_values, eps = 0.01):\n",
    "    # s is encoded in input, a is encoded in output\n",
    "    u = np.random.rand()\n",
    "    if u > eps:\n",
    "        return np.argmax(q_values[s])\n",
    "    else:\n",
    "        return np.random.randint(0, len(q_values[s]))\n",
    "    \n",
    "def update_q(s, a, r, sp, ap, q_values, gamma = 1):\n",
    "    q_values[s,a] = r + gamma*q_values[sp,ap]\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@\n",
    "# Environment variables\n",
    "#@@@@@@@@@@@@@@@@@@@@@@\n",
    "num_players = 1\n",
    "map_size = 7 # 7 x 7 map\n",
    "tot_turns = 400 # number of turns for each episode\n",
    "\n",
    "#@@@@@@@@@@@@@@@@\n",
    "# State variables\n",
    "#@@@@@@@@@@@@@@@@\n",
    "h_lev = 3 # halite levels\n",
    "n_cells = map_size**2 # number of cells in a square map\n",
    "n_states = n_cells*h_lev**6*4\n",
    "n_actions = 5 # no dropoffs, 1 action for staying still, 4 for moving in the cardinal directions\n",
    "print(\"Total number of states to be experienced: \", n_states)\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@\n",
    "# Learning parameters\n",
    "#@@@@@@@@@@@@@@@@@@@@\n",
    "n_batch = 100 # number of episodes in an epoch\n",
    "max_epochs = 1000 # max number of epochs played before stopping\n",
    "discount_factor = 1 - 1/tot_turns #train ships as if each turn has a probability of 1/tot_turns of ending the game \n",
    "STD_REWARD = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c696c9b3c764917bb511fb500f807fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237d9540caec4e29882ad411e16746b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 1:  0.5248300000000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0e3bb772924bcc81380584876f5b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 2:  0.7805099999999999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b687e5054a374f10b8fa6fad4efe3fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 3:  0.8261900000000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7c314829704155885da0e632a01755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 4:  0.7928700000000012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3224b69c994511a5c827b411b0dae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 5:  0.7274200000000012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3197a63f7d4f432495a401b0263da10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 6:  0.864130000000002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d04eb070536433f87814d78090c3002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 7:  0.9929400000000029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e319c1b91dc84fc6a6166ab7ff81d0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 8:  0.9712100000000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dadac65f53645b698306ba624e9e82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 9:  1.1098900000000014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed31884a1c024ff1b930a53e0c218e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 10:  1.1676800000000023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d12f5bfecf49148dd784313baed542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 11:  1.390470000000005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d5861d8da144e9b534a06a702202dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 12:  1.1995800000000028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154796e6fcd04732a2c87b5b4ef89fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 13:  1.2799300000000047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b40268c0ac404694d83b1045bcbedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 14:  1.220710000000003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4826820900da4a36b07bc211a6d68f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 15:  0.9845600000000022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69012685ec874e459d72edf5dda16b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-d403bac17d21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mnew_halite\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_halite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m#print(\"Reward obtained: \", r)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0msp_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_lev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_lev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m \u001b[0;31m# cumulative reward of the episode, obsolete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-4c62b37e2260>\u001b[0m in \u001b[0;36mencode_state\u001b[0;34m(state, map_size, h_lev, n_actions, debug)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ship position encoded in [0,%d]: \"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_size\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhalvec_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_halite_vec_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mhalvec_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalvec_dec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# halite vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-9cf3648ef254>\u001b[0m in \u001b[0;36mget_halite_vec_dec\u001b[0;34m(state, q_number, map_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmap_halite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mhalite_quant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhalite_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_halite\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# quantize halite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mhalite_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-9cf3648ef254>\u001b[0m in \u001b[0;36mhalite_quantization\u001b[0;34m(h, q_number)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalite_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# h can either be a scalar or a matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_number\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [10, 100, 1000] = [10^1, 10^2, 10^3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mh_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mh_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/function_base.py\u001b[0m in \u001b[0;36mlogspace\u001b[0;34m(start, stop, num, endpoint, base, dtype)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/function_base.py\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(start, stop, num, endpoint, retstep, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@\n",
    "# Learning variables\n",
    "#@@@@@@@@@@@@@@@@@@@\n",
    "q_values = np.zeros((n_states,n_actions)) #initialize to zero\n",
    "reward_score = np.zeros(max_epochs)\n",
    "epochs = 0\n",
    "\n",
    "from tqdm import tnrange\n",
    "\n",
    "for k in tnrange(max_epochs):\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    reward_progress = np.zeros(n_batch) # bunch of 100 episodes\n",
    "    eps = 0.5 # starting value of epsilon\n",
    "    # generate an adaptive epsilon greedy algorithm, calibrated in order to have epsilon = 10^-4 at the last epoch\n",
    "    epsilons = np.array(list(map(lambda i : eps*np.exp(-i*3*np.log(10)/max_epochs), np.arange(0,max_epochs+1))))\n",
    "    \n",
    "    for i in tnrange(n_batch):\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        # here starts an episode\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        env = Env.HaliteEnv(num_players, map_size, episode_lenght = tot_turns) # init environment\n",
    "        steps = 0\n",
    "        reward = 0\n",
    "        \n",
    "        # first mandatory step\n",
    "        steps = steps + 1\n",
    "        #print(\"\\nStep number %d:\"%steps)\n",
    "        action_matrix = np.full((map_size,map_size), -1) # no ship, no action\n",
    "        shipyard_action = 1 # initially always choose to create a ship\n",
    "        # returns the matricial state, the array of players halite and a flag that is true if it's the final turn\n",
    "        state, players_halite, finish, _ = env.step(action_matrix, makeship = shipyard_action) \n",
    "        #print(\"Cargo layer: \\n\", state[:,:,2])\n",
    "        current_halite = players_halite[0][0]\n",
    "        s_enc = encode_state(state, map_size = map_size, h_lev = h_lev, n_actions = n_actions, debug=False)\n",
    "        \n",
    "        while True:\n",
    "            steps = steps + 1\n",
    "            #print(\"\\nStep number %d:\"%steps)\n",
    "            #print(\"Current halite: \", current_halite)\n",
    "            a_enc = e_greedy_policy(s_enc, q_values, eps = epsilons[epochs])\n",
    "            a_mat = scalar_to_matrix_action(a_enc, state, map_size = map_size)\n",
    "            \n",
    "            # submit the action and get the new state\n",
    "            state, players_halite, finish, _ = env.step(a_mat, makeship = False) \n",
    "            #print(\"Cargo layer: \\n\", state[:,:,2])\n",
    "            new_halite = players_halite[0][0]\n",
    "            #print(\"New halite: \", new_halite)\n",
    "            # compute the 1--ship reward as the halite increment of the player divided by the max halite \n",
    "            # plus a standard negative reward \n",
    "            if new_halite == current_halite:\n",
    "                r = STD_REWARD \n",
    "            else:\n",
    "                #print('Halite deposited: ', new_halite - current_halite)\n",
    "                r =  (new_halite - current_halite)/1000\n",
    "            #print(\"Reward obtained: \", r)\n",
    "            sp_enc = encode_state(state, map_size = map_size, h_lev = h_lev, n_actions = n_actions, debug=False)\n",
    "            reward = reward + r # cumulative reward of the episode, obsolete\n",
    "            \n",
    "            a_temp_enc = greedy_policy(sp_enc, q_values) # simulate the best action in the new state (before update)\n",
    "            \n",
    "            # update Q-values\n",
    "            q_values = update_q(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, gamma = discount_factor)\n",
    "            \n",
    "            # update states and halite\n",
    "            s_enc = sp_enc\n",
    "            current_halite = new_halite\n",
    "            \n",
    "            if (finish == True) or (steps >= 400):\n",
    "                #print(\"End episode.\")\n",
    "                reward_progress[i] = reward\n",
    "                break\n",
    "        #break # play just 1 episode\n",
    "                \n",
    "    #break # play just 1 epoch\n",
    "\n",
    "    print(\"Average reward per episode in epoch %d: \"%epochs, reward_progress.mean())\n",
    "    reward_score[epochs-1] = reward_progress.mean()\n",
    "\n",
    "    if epochs >= max_epochs:\n",
    "        print(\"Hey, I think you've had enough! Let's stop here.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
